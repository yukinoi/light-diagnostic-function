{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wccSDKfhLpD6",
    "outputId": "39f5d817-6a14-4a1a-d079-7db09325a183",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install talos==0.6.4\n",
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LtcZfJtJC1n-",
    "outputId": "8a982397-fee8-4d5a-dc22-68f6e569c28e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Layer, Activation\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from talos.model.early_stopper import early_stopper\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import talos as ta\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import timeit\n",
    "from IPython.display import clear_output \n",
    "import os\n",
    "\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker\n",
    "\n",
    "\n",
    "class LIGHTVSigmoid(Layer):\n",
    "    def __init__(self, T, C, r, E, N0, **kwargs):\n",
    "        super(LIGHTVSigmoid, self).__init__(**kwargs)\n",
    "        self.T = K.cast_to_floatx(T)\n",
    "        self.r = K.cast_to_floatx(r)\n",
    "        self.C = K.cast_to_floatx(C)\n",
    "        self.E = K.cast_to_floatx(E)\n",
    "        self.N0 = K.cast_to_floatx(N0)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if K.cast(K.greater(inputs, self.T), K.floatx()) == 1:\n",
    "            temp = self.C\n",
    "            NT = temp/(1-(1-temp/self.N0)*K.exp(-self.r*inputs))\n",
    "            \n",
    "            temp = (self.r-self.E)*self.C/self.r \n",
    "            result = temp/(1-(1-temp/NT)*K.exp(-(self.r-self.E)*(inputs-self.T)))\n",
    "        else:\n",
    "            temp = self.C \n",
    "            result = temp/(1-(1-temp/self.N0)*K.exp(-self.r*inputs))\n",
    "        return result\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'T': float(self.T), 'r': float(self.r), 'C': float(self.C), 'E': float(self.E), 'N0': float(self.N0)}\n",
    "        base_config = super(LIGHTVSigmoid, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    \n",
    "class LIGHTGSigmoid(Layer):\n",
    "    def __init__(self, T, C, r, E, N0, **kwargs):\n",
    "        super(LIGHTGSigmoid, self).__init__(**kwargs)\n",
    "        self.T = K.cast_to_floatx(T)\n",
    "        self.r = K.cast_to_floatx(r)\n",
    "        self.C = K.cast_to_floatx(C)\n",
    "        self.E = K.cast_to_floatx(E)\n",
    "        self.N0 = K.cast_to_floatx(N0)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if K.cast(K.greater(inputs, self.T), K.floatx()) == 1:\n",
    "            temp = np.log(self.N0/self.C) \n",
    "            NT = self.C*K.exp(temp*K.exp(-self.r*inputs))\n",
    "            \n",
    "            temp = self.r*np.log(NT/self.C) + self.E  \n",
    "            result = self.C*K.exp((temp*K.exp(-self.r*(inputs-self.T))-self.E)/self.r)\n",
    "        else:\n",
    "            temp = np.log(self.N0/self.C) \n",
    "            result = self.C*K.exp(temp*K.exp(-self.r*inputs))\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'T': float(self.T), 'r': float(self.r), 'C': float(self.C), 'E': float(self.E), 'N0': float(self.N0)}\n",
    "        base_config = super(LIGHTGSigmoid, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def binary_crossentropy_light(y_true, y_pred):\n",
    "    return K.mean(binary_crossentropy_light_tf(y_true, y_pred), axis=-1)\n",
    "\n",
    "def binary_crossentropy_light_tf(target, output, from_logits=False):\n",
    "    \n",
    "    if not from_logits:\n",
    "        # transform back to logits\n",
    "        _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "        output = tf.log(output / (1 - output))\n",
    "\n",
    "    return tf.nn.sigmoid_cross_entropy_with_logits(labels = target,\n",
    "                                                   logits = output)\n",
    "\n",
    "    \n",
    "def build_modelG(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim = x_train.shape[1]))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dense(num_h))  \n",
    "    #model.add(Dense(1)) \n",
    "    model.add(LIGHTGSigmoid(T=params['T'], C=params['C'], r=params['r'], E = params['E'], N0 = params['N0']))\n",
    "    model.compile(loss = binary_crossentropy_light, optimizer = 'sgd', metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='model.weights.bestG.hdf5', verbose = 0, save_best_only=False)\n",
    "\n",
    "    out = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=random_search_epochs,\n",
    "                    callbacks=[checkpointer,early_stopper(random_search_epochs, patience=10)],\n",
    "                    verbose = 0,\n",
    "                    validation_data=[x_val, y_val])\n",
    "    \n",
    "    return out, model\n",
    "\n",
    "def build_modelV(x_train, y_train, x_val, y_val, params):\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim = x_train.shape[1]))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dense(num_h))  \n",
    "    #model.add(Dense(1))\n",
    "    model.add(LIGHTVSigmoid(T=params['T'], C=params['C'], r=params['r'], E = params['E'], N0 = params['N0']))\n",
    "    model.compile(loss = binary_crossentropy_light, optimizer = 'sgd', metrics=['acc'])\n",
    "\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='model.weights.bestV.hdf5', verbose = 0, save_best_only=False)\n",
    "\n",
    "    out = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=random_search_epochs,\n",
    "                    callbacks=[checkpointer,early_stopper(random_search_epochs, patience=10)],\n",
    "                    verbose = 0,\n",
    "                    validation_data=[x_val, y_val])\n",
    "    \n",
    "    return out, model\n",
    "\n",
    "def upload_dataset_mnist(n_samples, test_size, target):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "        \n",
    "    y_train = (y_train == target)\n",
    "    y_test = (y_test == target)\n",
    "    \n",
    "    idx = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train = x_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "    x_train = x_train[:n_samples]\n",
    "    y_train = y_train[:n_samples]\n",
    "\n",
    "    idx = np.arange(x_test.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_test = x_test[idx]\n",
    "    y_test = y_test[idx]\n",
    "    x_test = x_test[:round(n_samples*test_size)]\n",
    "    y_test = y_test[:round(n_samples*test_size)]\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])\n",
    "\n",
    "\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def upload_dataset_fmnist(n_samples, test_size, target):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    \n",
    "    y_train = (y_train == target)\n",
    "    y_test = (y_test == target)\n",
    "        \n",
    "    idx = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train = x_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "    x_train = x_train[:n_samples]\n",
    "    y_train = y_train[:n_samples]\n",
    "\n",
    "    idx = np.arange(x_test.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_test = x_test[idx]\n",
    "    y_test = y_test[idx]\n",
    "    x_test = x_test[:round(n_samples*test_size)]\n",
    "    y_test = y_test[:round(n_samples*test_size)]\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])\n",
    "\n",
    "\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def grayscale(data, dtype='float32'):\n",
    "    # luma coding weighted average in video systems\n",
    "    r, g, b = np.asarray(.3, dtype=dtype), np.asarray(.59, dtype=dtype), np.asarray(.11, dtype=dtype)\n",
    "    rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]\n",
    "    # add channel dimension\n",
    "    rst = np.expand_dims(rst, axis=3)\n",
    "    return rst\n",
    "\n",
    "\n",
    "def upload_dataset_cifar10(n_samples, test_size, target):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    \n",
    "    x_train = grayscale(x_train)\n",
    "    x_test = grayscale(x_test)\n",
    "    \n",
    "    y_train = (y_train == target)\n",
    "    y_test = (y_test == target)\n",
    "    \n",
    "    idx = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train = x_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "    x_train = x_train[:n_samples]\n",
    "    y_train = y_train[:n_samples]\n",
    "\n",
    "    idx = np.arange(x_test.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_test = x_test[idx]\n",
    "    y_test = y_test[idx]\n",
    "    x_test = x_test[:round(n_samples*test_size)]\n",
    "    y_test = y_test[:round(n_samples*test_size)]\n",
    "\n",
    "    x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1]*x_train.shape[2]*x_train.shape[3]))\n",
    "    x_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3]))\n",
    "\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_hyper_param_from_csv(r, s):\n",
    "    path = os.path.abspath(os.getcwd()+'/'+s+str(r))\n",
    "    file_extract = [(root, max(fname for fname in files if fname.endswith(\".csv\"))) for root, dirs, files in os.walk(path)]   \n",
    "    extract_name = file_extract[0][1]\n",
    "    \n",
    "    return extract_name, path\n",
    "\n",
    "\n",
    "def optimize_hyper_param(x, y, opt, model, r, fraction_limit, s):\n",
    "\n",
    "    if opt == 'Er':\n",
    "        r_range = [4.08]\n",
    "        E_range = [6.4]\n",
    "    elif opt == 'E':\n",
    "        r_range = [1]\n",
    "        E_range = (0.0,20,5)\n",
    "    elif opt == 'r': \n",
    "        r_range = (0.1,20,5)\n",
    "        E_range = [0]\n",
    "\n",
    "    p = {'T': (1.0,3.0,3),\n",
    "         'C': [1],\n",
    "         'r': r_range,\n",
    "         'E': E_range,\n",
    "         'N0': (0.2,0.8,5)\n",
    "    }\n",
    "\n",
    "    \n",
    "    h = ta.Scan(x, y, params=p,\n",
    "                model=model,\n",
    "                experiment_name=s+str(r),\n",
    "                fraction_limit = fraction_limit)\n",
    "    \n",
    "    extract_name, path = extract_hyper_param_from_csv(r, s)\n",
    "    \n",
    "    opt_hyper_param = pd.read_csv(path + '/' + extract_name)\n",
    "    opt_hyper_param = opt_hyper_param.round(4)\n",
    "    opt_hyper_param_sort = opt_hyper_param.sort_values('val_acc', ascending=False)\n",
    "    \n",
    "    T = opt_hyper_param_sort['T'].iloc[0]\n",
    "    C = opt_hyper_param_sort['C'].iloc[0]\n",
    "    r = opt_hyper_param_sort['r'].iloc[0]\n",
    "    E = opt_hyper_param_sort['E'].iloc[0]\n",
    "    N0 = opt_hyper_param_sort['N0'].iloc[0]\n",
    "    acc = opt_hyper_param_sort['val_acc'].iloc[0]\n",
    "       \n",
    "    return T, C, r, E, N0, acc\n",
    "\n",
    "def build_adam_model(num_h):\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim = x_train.shape[1]))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dense(num_h))  \n",
    "    #model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_adagrad_model(num_h):\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim = x_train.shape[1]))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dense(num_h))  \n",
    "    #model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_sgd_model(num_h):\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim = x_train.shape[1]))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dense(num_h))  \n",
    "    #model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_best_model(LIGHT_type, T, C, r, E, N0, num_h):\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1, input_dim = x_train.shape[1]))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dense(num_h))  \n",
    "    #model.add(Dense(1))\n",
    "    model.add(LIGHT_type(T=T, C=C, r=r, E = E, N0 = N0))\n",
    "    model.compile(loss=binary_crossentropy_light, optimizer='sgd', metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_light_models(x_train, y_train, x_test, y_test, batch_size, epochs, num_h, T_v, C_v, r_v, E_v, N0_v, T_g, C_g, r_g, E_g, N0_g):\n",
    "    tensorboard = TensorBoard(log_dir='./logs',\n",
    "                 histogram_freq=0,batch_size=batch_size,\n",
    "                 write_graph=False, \n",
    "                 write_images=False)\n",
    "\n",
    "    model_bestV = build_best_model(LIGHTVSigmoid, T_v, C_v, r_v, E_v, N0_v, num_h)\n",
    "    model_bestG = build_best_model(LIGHTGSigmoid, T_g, C_g, r_g, E_g, N0_g, num_h)\n",
    "\n",
    "    \n",
    "    out_bestV = model_bestV.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size = batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "\n",
    "    out_bestG = model_bestG.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size = batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "      \n",
    "    return out_bestV, out_bestG\n",
    "\n",
    "def train_default_models(x_train, y_train, x_test, y_test, batch_size, epochs, num_h):\n",
    "    tensorboard = TensorBoard(log_dir='./logs',\n",
    "                 histogram_freq=0,batch_size=batch_size,\n",
    "                 write_graph=False, \n",
    "                 write_images=False)\n",
    "\n",
    "    model_adam = build_adam_model(num_h)\n",
    "    model_adagrad = build_adagrad_model(num_h)\n",
    "    model_sgd = build_sgd_model(num_h)\n",
    "\n",
    "\n",
    "    out_adam = model_adam.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "    \n",
    "    out_adagrad = model_adagrad.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "    \n",
    "    out_sgd = model_sgd.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "      \n",
    "    return out_adam, out_adagrad, out_sgd\n",
    "\n",
    "def train_models(x_train, y_train, x_test, y_test, batch_size, epochs, num_h, T_v, C_v, r_v, E_v, N0_v, T_g, C_g, r_g, E_g, N0_g):\n",
    "    tensorboard = TensorBoard(log_dir='./logs',\n",
    "                 histogram_freq=0,batch_size=batch_size,\n",
    "                 write_graph=False, \n",
    "                 write_images=False)\n",
    "\n",
    "    model_bestV = build_best_model(LIGHTVSigmoid, T_v, C_v, r_v, E_v, N0_v, num_h)\n",
    "    model_bestG = build_best_model(LIGHTGSigmoid, T_g, C_g, r_g, E_g, N0_g, num_h)\n",
    "    model_adam = build_adam_model(num_h)\n",
    "    model_adagrad = build_adagrad_model(num_h)\n",
    "    model_sgd = build_sgd_model(num_h)\n",
    "\n",
    "    \n",
    "    out_bestV = model_bestV.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size = batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "\n",
    "    out_bestG = model_bestG.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size = batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "\n",
    "    out_adam = model_adam.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "    \n",
    "    out_adagrad = model_adagrad.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "    \n",
    "    out_sgd = model_sgd.fit(x_train,\n",
    "         y_train,\n",
    "         batch_size=batch_size, shuffle=False,\n",
    "         epochs=epochs,\n",
    "         validation_data=(x_test, y_test),\n",
    "         callbacks=[tensorboard], verbose = 0)\n",
    "      \n",
    "    return out_bestV, out_bestG, out_adam, out_adagrad, out_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup\n",
    "random_state = 41\n",
    "    \n",
    "name = 'cifar10'\n",
    "target = 5\n",
    "num_samples_total = 1000\n",
    "\n",
    "test_size = 0.2\n",
    "batch_size = 75\n",
    "\n",
    "opt_options = ['Er', 'E', 'r']\n",
    "\n",
    "fraction_limit = 0.075 \n",
    "\n",
    "random_search_epochs = 1\n",
    "num_h = 0\n",
    "L = 0\n",
    "\n",
    "epoch = 1500\n",
    "rounds = 5\n",
    "\n",
    "# run experiments\n",
    "hyper_round_V = []\n",
    "hyper_round_G = []\n",
    "\n",
    "out_round_V = []\n",
    "out_round_G = []\n",
    "out_round_sgd = []\n",
    "out_round_adam = []\n",
    "out_round_adagrad = []\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for r in range(1,rounds+1):\n",
    "\n",
    "    # generate datasets\n",
    "    if name == 'mnist':\n",
    "        x_train, x_test, y_train, y_test = upload_dataset_mnist(num_samples_total, test_size, target)\n",
    "    elif name == 'fmnist':\n",
    "        x_train, x_test, y_train, y_test = upload_dataset_fmnist(num_samples_total, test_size, target)\n",
    "    elif name == 'cifar10':\n",
    "        x_train, x_test, y_train, y_test = upload_dataset_cifar10(num_samples_total, test_size, target)\n",
    "    \n",
    "    for pi in opt_options:\n",
    "        # optimize hyperparameters\n",
    "        T_v, C_v, r_v, E_v, N0_v, acc_v  = optimize_hyper_param(x_train, y_train, pi, build_modelV, r, fraction_limit, 'hypersV')    \n",
    "        T_g, C_g, r_g, E_g, N0_g, acc_g = optimize_hyper_param(x_train, y_train, pi, build_modelG, r, fraction_limit, 'hypersG')\n",
    "    \n",
    "        hyper_round_V.append([T_v, C_v, r_v, E_v, N0_v, acc_v])\n",
    "        hyper_round_V_dataframe = pd.DataFrame(hyper_round_V,\n",
    "                                           columns =['T', 'C', 'r', 'E', 'N0', 'val_acc']) \n",
    "        hyper_round_G.append([T_g, C_g, r_g, E_g, N0_g, acc_g])\n",
    "        hyper_round_G_dataframe = pd.DataFrame(hyper_round_G,\n",
    "                                           columns =['T', 'C', 'r', 'E', 'N0', 'val_acc']) \n",
    "\n",
    "        print(hyper_round_V_dataframe)\n",
    "        print(hyper_round_G_dataframe)\n",
    "        \n",
    "        out_V, out_G = train_light_models(x_train, y_train, x_test, y_test, batch_size, epoch, num_h, T_v, C_v, r_v, E_v, N0_v, T_g, C_g, r_g, E_g, N0_g)      \n",
    "        \n",
    "        out_V = pd.DataFrame.from_dict(out_V.history)\n",
    "        out_G = pd.DataFrame.from_dict(out_G.history)\n",
    "           \n",
    "        out_round_V.append(out_V)\n",
    "        out_round_G.append(out_G)\n",
    "       \n",
    "        \n",
    "    out_adam, out_adagrad, out_sgd = train_default_models(x_train, y_train, x_test, y_test, batch_size, epoch, num_h)      \n",
    "    out_adam = pd.DataFrame.from_dict(out_adam.history)\n",
    "    out_adagrad = pd.DataFrame.from_dict(out_adagrad.history)\n",
    "    out_sgd = pd.DataFrame.from_dict(out_sgd.history)\n",
    "    \n",
    "    out_round_adam.append(out_adam)\n",
    "    out_round_adagrad.append(out_adagrad)\n",
    "    out_round_sgd.append(out_sgd)\n",
    "    \n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    if (r/rounds*100) < 5:\n",
    "        expected_time = 'Calculating ...'\n",
    "    else:\n",
    "        time_perc = timeit.default_timer()\n",
    "        expected_time = np.round( ( (time_perc-start)/(r/rounds) )/60, 2)\n",
    "    print('Current progress:', np.round(r/rounds*100, 2), '%')\n",
    "    print('Current run time:', np.round((stop-start)/60, 2), 'mins')\n",
    "    print('Expected run time:', expected_time, 'mins')\n",
    "        \n",
    "out_round_V = pd.concat(out_round_V, axis = 1)\n",
    "out_round_G = pd.concat(out_round_G, axis = 1)\n",
    "out_round_adam = pd.concat(out_round_adam, axis = 1)\n",
    "out_round_adagrad = pd.concat(out_round_adagrad, axis = 1)\n",
    "out_round_sgd = pd.concat(out_round_sgd, axis = 1)\n",
    "\n",
    "out_round_V = pd.concat([out_round_V, hyper_round_V_dataframe], axis= 1)\n",
    "out_round_G = pd.concat([out_round_G, hyper_round_G_dataframe], axis= 1)\n",
    "  \n",
    "pd.DataFrame.from_dict(out_round_V).to_csv(name+ str(target)+'_nt_V_dl'+str(num_h)+'_L'+str(L)+'.csv',index=False)\n",
    "pd.DataFrame.from_dict(out_round_G).to_csv(name + str(target)+'_nt_G_dl'+str(num_h)+'_L'+str(L)+'.csv',index=False)\n",
    "pd.DataFrame.from_dict(out_round_adam).to_csv(name + str(target)+'_nt_adam_dl'+str(num_h)+'_L'+str(L)+'.csv',index=False)\n",
    "pd.DataFrame.from_dict(out_round_adagrad).to_csv(name + str(target)+'_nt_adagrad_dl'+str(num_h)+'_L'+str(L)+'.csv',index=False)\n",
    "pd.DataFrame.from_dict(out_round_sgd).to_csv(name + str(target)+'_nt_sgd_dl'+str(num_h)+'_L'+str(L)+'.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "complete_separation_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
